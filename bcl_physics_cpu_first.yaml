base_model: Qwen/Qwen2.5-Coder-7B

# Force CPU loading first
device_map: {"": "cpu"}  # Load to CPU first
model_kwargs:
  torch_dtype: float16
  trust_remote_code: true

datasets:
  - path: ./bcl_complete_training.jsonl
    type: alpaca

output_dir: ./outputs/bcl_physics_gpu_cpu_first
val_set_size: 0.05

# Smaller settings to ensure it works
sequence_len: 512
micro_batch_size: 1
gradient_accumulation_steps: 8

# Simple LoRA
adapter: lora
lora_r: 16
lora_alpha: 32
lora_target_modules:
  - q_proj
  - v_proj

# Basic training
num_epochs: 1
learning_rate: 0.0002
warmup_steps: 20
logging_steps: 10
eval_steps: 50
save_steps: 100

# GPU settings
fp16: true
gradient_checkpointing: true

# Minimal config
chat_template: qwen3
deepspeed:
flash_attention: false
sample_packing: false
group_by_length: false
torch_compile: false