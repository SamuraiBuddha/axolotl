base_model: Qwen/Qwen2.5-Coder-7B
# 7B version with GPU acceleration

strict: false

chat_template: qwen3
datasets:
  - path: ./bcl_training_data_fixed.jsonl
    type: alpaca
val_set_size: 0.05
output_dir: ./outputs/bcl_finetune
dataset_prepared_path: last_run_prepared

sequence_len: 2048  # Back to full length with GPU
sample_packing: false
eval_sample_packing: false
pad_to_sequence_len: false

load_in_4bit: true  # Re-enabled for GPU
adapter: qlora  # Back to QLoRA
lora_r: 32  # Full rank again
lora_alpha: 64
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - down_proj
  - up_proj
lora_mlp_kernel: false  # Still no Triton on Windows
lora_qkv_kernel: false
lora_o_kernel: false

wandb_project: bcl-finetune
wandb_entity: 
wandb_watch:
wandb_name: qwen3-bcl-physics
wandb_log_model:

gradient_accumulation_steps: 4
micro_batch_size: 2  # Back to 2 with GPU
num_epochs: 3  # Full 3 epochs now feasible
optimizer: adamw_torch_4bit  # 4-bit optimizer
lr_scheduler: cosine
learning_rate: 0.0002

bf16: true  # GPU supports this
tf32: true  # A4000 supports this

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
resume_from_checkpoint:
logging_steps: 10
flash_attention: false  # Still no Triton on Windows

warmup_steps: 100
evals_per_epoch: 4
saves_per_epoch: 1
weight_decay: 0.01
special_tokens: